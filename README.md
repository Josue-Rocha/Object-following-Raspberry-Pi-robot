# Object-following-Raspberry-Pi-robot

This project will be broken down into five distinct components: Hardware Setup, Object Detection & Tracking, Data Collection & Training, Real-Time Control System, and Integration & Testing. 

## Component 1: Hardware Setup
For starters, I will be using a Raspberry Pi 4 Computer Desktop Kit which has been provided. While this kit contains many crucial components, I will have to find ways to get my hands on a set of actual robot parts. This includes, but is not limited to, wheels, car chassis, motor, motor controllers, wires, a battery, and most obviously...a camera. The design of the physical robot itself might have to be a bit of an afterthought as long at it can perform its basic functions. Afterall, the robot itself is not the point of the project.
## Component 2: Object Detection & Tracking
For this component, I was hoping to implement a deep learning-based detection for object tracking. Since it is still early on in the process, I am still not sure which pre-existing model I should use as the basis for this component. I know certain models will prioritize efficiency over accuracy and vice versa. Potential models include: YOLOv8 Nano, MobileNet SSD, and EfficientDet-Lite.  Additionally, these models can often be very computationally heavy, so I know I may have to use a resource such as TensorFlow Lite to convert it into an optimized format for the Raspberry Pi.
## Component 3: Data Collection & Training
For this component, I was planning on gathering my own custom set of training data. This data would likely have to consist of thousands of images of different colored balls. In order to increase the robustness of the model, I could diversify the data set by taking photos from different angles, distances, with different lighting conditions, and different background. Obviously the data will need to divided into a roughly 70-20-10 split between training, validation, and final testing. I also know that there are resources such as Roboflow and LabelImg which can be used for data annotation to help the model during the training process. As of right now, my plan would be to train the model on the custom dataset using either Google Colab or a local GPU machine.
## Component 4: Real-Time Control System
For this component, I was planning on using PID control to adjust robot movement based on ball position. The basic idea being if the ball moves to the left, then the robot should move to the left. I would have to learn more about using PID controls to make the Raspberry Pi robot move in real time. 
## Component 5: Integration & Testing
Lastly, one of the biggest tasks for me will be to figure out how to connect the object tracking model and the real-time control system. I will have to find a way to take the results of what the tracking model picked up (i.e. it detected that the ball moved to the left) and translate those results into real-time instructions for what the robot should do next (i.e. move to the left). One possible idea could be to consider an algorithm that constantly tries to keep the ball in the center of the robot's "vision" (i.e. center of the screen if we are looking through the camera). In this algorithm, the robot would make any adjustments necessary to reposition itself so that the ball will always remain in the center of the screen. Additionally, it could theoretically be expanded and used as a way for the robot to know when to move forward and backward. Since the ball will effectively be a circle on the screen, the algorthim can try to make sure the circle will always remains the same size. If the circle gets smaller, then the robot would know it has to move forward until it is the right size again. 
